Libraries
[66]
import tensorflow as tf
import tensorflow.keras as k
from tensorflow.keras import layers
from tensorflow.keras import backend as K
import tensorflow_datasets as tfds
import numpy as np
import os
from PIL import Image, ImageOps
import matplotlib.pyplot as plt


Data Preprocessing
[67]
def scale_normalize(image, label):
    image = tf.image.convert_image_dtype(image, tf.float32)
    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])
    return image, tf.one_hot(label, depth=8)

def gaussian_noise(image, label):
    noise = tf.random.normal(shape=tf.shape(image), mean=0.0, stddev=abs(np.random.normal(loc=0.0, scale=0.1)),dtype=tf.float32)
    image = tf.add(image, noise)
    return image, label
[68]
ds_train, ds_test = tfds.load('colorectal_histology', split=['train[:70%]', 'train[70%:]'],
              shuffle_files=True, as_supervised=True)

ds_train = ds_train.map(scale_normalize)
ds_train = ds_train.cache()
ds_train = ds_train.map(lambda image, label: (tf.image.random_flip_left_right(image), label))
ds_train = ds_train.map(lambda image, label: (tf.image.random_flip_up_down(image), label))
ds_train = ds_train.map(gaussian_noise)
ds_train = ds_train.shuffle(1000)
ds_train = ds_train.batch
[TRAIN]:  <RepeatDataset shapes: ((None, 128, 128, 3), (None, 8)), types: (tf.float32, tf.float32)>
[TEST]:  <BatchDataset shapes: ((None, 128, 128, 3), (None, 8)), types: (tf.float32, tf.float32)>
Autoencoder
[5]
def autoencoder_model(input_shape=(128, 128, 3), kernel_size=(3,3), pool_size=(2,2)):
    input_img = layers.Input(shape=input_shape)
    x = layers.Conv2D(128, kernel_size, activation='relu', padding='same')(input_img)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D(pool_size, padding='same')(x)
    x = layers.Conv2D(64, kernel_size, activation='relu', padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D(pool_size, padding='same')(x)
    x = l
[6]
autoencoder = autoencoder_model()

autoencoder.summary()
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 128, 128, 3)]     0         
_________________________________________________________________
conv2d (Conv2D)              (None, 128, 128, 128)     3584      
_________________________________________________________________
batch_normalization (BatchNo (None, 128, 128, 128)     512       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 64, 64, 128)       0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 64, 64, 64)        73792     
_________________________________________________________________
batch_normalization_1 (Batch (None, 64, 64, 64)        256       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 32, 32, 64)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 32, 32, 32)        18464     
_________________________________________________________________
batch_normalization_2 (Batch (None, 32, 32, 32)        128       
_________________________________________________________________
encoder (MaxPooling2D)       (None, 16, 16, 32)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 16, 16, 32)        9248      
_________________________________________________________________
batch_normalization_3 (Batch (None, 16, 16, 32)        128       
_________________________________________________________________
up_sampling2d (UpSampling2D) (None, 32, 32, 32)        0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 32, 32, 64)        18496     
_________________________________________________________________
batch_normalization_4 (Batch (None, 32, 32, 64)        256       
_________________________________________________________________
up_sampling2d_1 (UpSampling2 (None, 64, 64, 64)        0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 64, 64, 128)       73856     
_________________________________________________________________
batch_normalization_5 (Batch (None, 64, 64, 128)       512       
_________________________________________________________________
up_sampling2d_2 (UpSampling2 (None, 128, 128, 128)     0         
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 128, 128, 3)       3459      
=================================================================
Total params: 202,691
Trainable params: 201,795
Non-trainable params: 896
_________________________________________________________________
[7]
autoencoder.fit(ds_train_autoencoder,
                epochs=25,
                validation_data=ds_test_autoencoder, 
                callbacks=[
                           k.callbacks.TensorBoard(log_dir="./autoencoder/logs"), 
                           k.callbacks.ModelCheckpoint('./autoencoder/ckpt/autoencoder-{epoch:02d}-{val_loss:.4f}.hdf5', 
                                                       monitor='val_loss', 
                                                       save_best_only=Fa
Epoch 1/25
440/440 [==============================] - 147s 255ms/step - loss: 0.0309 - acc: 0.5450 - val_loss: 0.0187 - val_acc: 0.6578
Epoch 2/25
440/440 [==============================] - 108s 242ms/step - loss: 0.0182 - acc: 0.5864 - val_loss: 0.0072 - val_acc: 0.6688
Epoch 3/25
440/440 [==============================] - 108s 243ms/step - loss: 0.0166 - acc: 0.5897 - val_loss: 0.0072 - val_acc: 0.6983
Epoch 4/25
440/440 [==============================] - 108s 242ms/step - loss: 0.0161 - acc: 0.5944 - val_loss: 0.0045 - val_acc: 0.6971
Epoch 5/25
440/440 [==============================] - 108s 242ms/step - loss: 0.0159 - acc: 0.5932 - val_loss: 0.0056 - val_acc: 0.7142
Epoch 6/25
440/440 [==============================] - 108s 241ms/step - loss: 0.0155 - acc: 0.5962 - val_loss: 0.0063 - val_acc: 0.7119
Epoch 7/25
440/440 [==============================] - 109s 244ms/step - loss: 0.0152 - acc: 0.5965 - val_loss: 0.0075 - val_acc: 0.7009
Epoch 8/25
440/440 [==============================] - 109s 244ms/step - loss: 0.0152 - acc: 0.5968 - val_loss: 0.0501 - val_acc: 0.6168
Epoch 9/25
440/440 [==============================] - 109s 244ms/step - loss: 0.0151 - acc: 0.5959 - val_loss: 0.0051 - val_acc: 0.7211
Epoch 10/25
440/440 [==============================] - 109s 245ms/step - loss: 0.0148 - acc: 0.5969 - val_loss: 0.0041 - val_acc: 0.7110
Epoch 11/25
440/440 [==============================] - 109s 245ms/step - loss: 0.0147 - acc: 0.5971 - val_loss: 0.0048 - val_acc: 0.7431
Epoch 12/25
440/440 [==============================] - 109s 244ms/step - loss: 0.0147 - acc: 0.5972 - val_loss: 0.0073 - val_acc: 0.7135
Epoch 13/25
440/440 [==============================] - 109s 245ms/step - loss: 0.0145 - acc: 0.5983 - val_loss: 0.0046 - val_acc: 0.7558
Epoch 14/25
440/440 [==============================] - 109s 245ms/step - loss: 0.0142 - acc: 0.5989 - val_loss: 0.0143 - val_acc: 0.7397
Epoch 15/25
440/440 [==============================] - 109s 244ms/step - loss: 0.0143 - acc: 0.5995 - val_loss: 0.0191 - val_acc: 0.6452
Epoch 16/25
440/440 [==============================] - 109s 245ms/step - loss: 0.0150 - acc: 0.5960 - val_loss: 0.0047 - val_acc: 0.7737
Epoch 17/25
440/440 [==============================] - 109s 245ms/step - loss: 0.0142 - acc: 0.5989 - val_loss: 0.0036 - val_acc: 0.7698
Epoch 18/25
440/440 [==============================] - 109s 244ms/step - loss: 0.0141 - acc: 0.6002 - val_loss: 0.0033 - val_acc: 0.7588
Epoch 19/25
440/440 [==============================] - 109s 244ms/step - loss: 0.0138 - acc: 0.6018 - val_loss: 0.0029 - val_acc: 0.7713
Epoch 20/25
440/440 [==============================] - 109s 245ms/step - loss: 0.0137 - acc: 0.6016 - val_loss: 0.0033 - val_acc: 0.7397
Epoch 21/25
440/440 [==============================] - 109s 244ms/step - loss: 0.0138 - acc: 0.6018 - val_loss: 0.0031 - val_acc: 0.7687
Epoch 22/25
440/440 [==============================] - 109s 245ms/step - loss: 0.0138 - acc: 0.6018 - val_loss: 0.0041 - val_acc: 0.7453
Epoch 23/25
440/440 [==============================] - 109s 244ms/step - loss: 0.0138 - acc: 0.6017 - val_loss: 0.0028 - val_acc: 0.7653
Epoch 24/25
440/440 [==============================] - 109s 245ms/step - loss: 0.0136 - acc: 0.6021 - val_loss: 0.0029 - val_acc: 0.7819
Epoch 25/25
440/440 [==============================] - 110s 246ms/step - loss: 0.0137 - acc: 0.6020 - val_loss: 0.0035 - val_acc: 0.7458
<tensorflow.python.keras.callbacks.History at 0x7fdb50f91690>
[8]
autoencoder.evaluate(ds_test_autoencoder)
47/47 [==============================] - 4s 78ms/step - loss: 0.0035 - acc: 0.7458
[0.003536514937877655, 0.745845377445221]
[94]
# print(item.shape, predict.shape)

fig, axs = plt.subplots(1,2, figsize=(14,16))
axs[0].imshow(imgs[n])
axs[1].imshow(predict[n])

# plt.show()

Model fijado
[22]

def model(encoder, trainable=False):
    encoder.trainable = trainable

    model = k.Sequential([
        encoder,
        layers.Flatten(),
        layers.Dense(4096, activation="relu"),
        layers.Dense(4096, activation="relu"),
        layers.Dense(2042, activation="relu"),

[23]

# Obtain features from middle layer
autoencoder = k.models.load_model('./autoencoder/ckpt/autoencoder-24-0.0029.hdf5')

encoder = k.Model(inputs=autoencoder.input,
                  outputs=autoencoder.get_layer('encoder').output)

encoder.summary()

classifier_fixed = model(encoder)

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 128, 128, 3)]     0         
_________________________________________________________________
conv2d (Conv2D)              (None, 128, 128, 128)     3584      
_________________________________________________________________
batch_normalization (BatchNo (None, 128, 128, 128)     512       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 64, 64, 128)       0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 64, 64, 64)        73792     
_________________________________________________________________
batch_normalization_1 (Batch (None, 64, 64, 64)        256       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 32, 32, 64)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 32, 32, 32)        18464     
_________________________________________________________________
batch_normalization_2 (Batch (None, 32, 32, 32)        128       
_________________________________________________________________
encoder (MaxPooling2D)       (None, 16, 16, 32)        0         
=================================================================
Total params: 96,736
Trainable params: 96,288
Non-trainable params: 448
_________________________________________________________________
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
model_1 (Functional)         (None, 16, 16, 32)        96736     
_________________________________________________________________
flatten (Flatten)            (None, 8192)              0         
_________________________________________________________________
dense (Dense)                (None, 4096)              33558528  
_________________________________________________________________
dense_1 (Dense)              (None, 4096)              16781312  
_________________________________________________________________
dense_2 (Dense)              (None, 2042)              8366074   
_________________________________________________________________
dense_3 (Dense)              (None, 2042)              4171806   
_________________________________________________________________
dense_4 (Dense)              (None, 1024)              2092032   
_________________________________________________________________
dense_5 (Dense)              (None, 1024)              1049600   
_________________________________________________________________
dense_6 (Dense)              (None, 512)               524800    
_________________________________________________________________
dense_7 (Dense)              (None, 512)               262656    
_________________________________________________________________
dense_8 (Dense)              (None, 8)                 4104      
=================================================================
Total params: 66,907,648
Trainable params: 66,810,912
Non-trainable params: 96,736
_________________________________________________________________
[24]
classifier_fixed.fit(ds_train,
                epochs=25,
                validation_data=ds_test, 
                callbacks=[
                           k.callbacks.TensorBoard(log_dir="./classifier-fixed/logs"), 
                           k.callbacks.ModelCheckpoint('./classifier-fixed/ckpt/classifier-fixed-{epoch:02d}-{val_loss:.4f}.hdf5', 
                                                       monitor='val_loss', 
                                                       save_best_only=False,
Epoch 1/25
440/440 [==============================] - 49s 105ms/step - loss: 0.1023 - acc: 0.3436 - categorical_accuracy: 0.3436 - val_loss: 0.0780 - val_acc: 0.5920 - val_categorical_accuracy: 0.5920
Epoch 2/25
440/440 [==============================] - 46s 100ms/step - loss: 0.0664 - acc: 0.6573 - categorical_accuracy: 0.6573 - val_loss: 0.0540 - val_acc: 0.7060 - val_categorical_accuracy: 0.7060
Epoch 3/25
440/440 [==============================] - 45s 99ms/step - loss: 0.0460 - acc: 0.7572 - categorical_accuracy: 0.7572 - val_loss: 0.0579 - val_acc: 0.6680 - val_categorical_accuracy: 0.6680
Epoch 4/25
440/440 [==============================] - 46s 101ms/step - loss: 0.0387 - acc: 0.7974 - categorical_accuracy: 0.7974 - val_loss: 0.0478 - val_acc: 0.7207 - val_categorical_accuracy: 0.7207
Epoch 5/25
440/440 [==============================] - 46s 102ms/step - loss: 0.0337 - acc: 0.8264 - categorical_accuracy: 0.8264 - val_loss: 0.0472 - val_acc: 0.7273 - val_categorical_accuracy: 0.7273
Epoch 6/25
440/440 [==============================] - 45s 99ms/step - loss: 0.0306 - acc: 0.8408 - categorical_accuracy: 0.8408 - val_loss: 0.0487 - val_acc: 0.7147 - val_categorical_accuracy: 0.7147
Epoch 7/25
440/440 [==============================] - 46s 102ms/step - loss: 0.0286 - acc: 0.8547 - categorical_accuracy: 0.8547 - val_loss: 0.0466 - val_acc: 0.7367 - val_categorical_accuracy: 0.7367
Epoch 8/25
440/440 [==============================] - 46s 101ms/step - loss: 0.0264 - acc: 0.8675 - categorical_accuracy: 0.8675 - val_loss: 0.0444 - val_acc: 0.7413 - val_categorical_accuracy: 0.7413
Epoch 9/25
440/440 [==============================] - 47s 102ms/step - loss: 0.0234 - acc: 0.8869 - categorical_accuracy: 0.8869 - val_loss: 0.0591 - val_acc: 0.6840 - val_categorical_accuracy: 0.6840
Epoch 10/25
440/440 [==============================] - 46s 102ms/step - loss: 0.0227 - acc: 0.8877 - categorical_accuracy: 0.8877 - val_loss: 0.0469 - val_acc: 0.7380 - val_categorical_accuracy: 0.7380
Epoch 11/25
440/440 [==============================] - 47s 103ms/step - loss: 0.0206 - acc: 0.9034 - categorical_accuracy: 0.9034 - val_loss: 0.0562 - val_acc: 0.7013 - val_categorical_accuracy: 0.7013
Epoch 12/25
440/440 [==============================] - 47s 102ms/step - loss: 0.0187 - acc: 0.9143 - categorical_accuracy: 0.9143 - val_loss: 0.0457 - val_acc: 0.7407 - val_categorical_accuracy: 0.7407
Epoch 13/25
440/440 [==============================] - 47s 102ms/step - loss: 0.0172 - acc: 0.9193 - categorical_accuracy: 0.9193 - val_loss: 0.0465 - val_acc: 0.7360 - val_categorical_accuracy: 0.7360
Epoch 14/25
440/440 [==============================] - 47s 102ms/step - loss: 0.0154 - acc: 0.9325 - categorical_accuracy: 0.9325 - val_loss: 0.0501 - val_acc: 0.7280 - val_categorical_accuracy: 0.7280
Epoch 15/25
440/440 [==============================] - 46s 101ms/step - loss: 0.0142 - acc: 0.9375 - categorical_accuracy: 0.9375 - val_loss: 0.0515 - val_acc: 0.7287 - val_categorical_accuracy: 0.7287
Epoch 16/25
440/440 [==============================] - 46s 101ms/step - loss: 0.0137 - acc: 0.9391 - categorical_accuracy: 0.9391 - val_loss: 0.0401 - val_acc: 0.7833 - val_categorical_accuracy: 0.7833
Epoch 17/25
440/440 [==============================] - 46s 101ms/step - loss: 0.0127 - acc: 0.9443 - categorical_accuracy: 0.9443 - val_loss: 0.0457 - val_acc: 0.7560 - val_categorical_accuracy: 0.7560
Epoch 18/25
440/440 [==============================] - 47s 102ms/step - loss: 0.0117 - acc: 0.9489 - categorical_accuracy: 0.9489 - val_loss: 0.0440 - val_acc: 0.7567 - val_categorical_accuracy: 0.7567
Epoch 19/25
440/440 [==============================] - 46s 102ms/step - loss: 0.0107 - acc: 0.9521 - categorical_accuracy: 0.9521 - val_loss: 0.0503 - val_acc: 0.7227 - val_categorical_accuracy: 0.7227
Epoch 20/25
440/440 [==============================] - 46s 102ms/step - loss: 0.0102 - acc: 0.9553 - categorical_accuracy: 0.9553 - val_loss: 0.0428 - val_acc: 0.7640 - val_categorical_accuracy: 0.7640
Epoch 21/25
440/440 [==============================] - 46s 101ms/step - loss: 0.0088 - acc: 0.9625 - categorical_accuracy: 0.9625 - val_loss: 0.0481 - val_acc: 0.7380 - val_categorical_accuracy: 0.7380
Epoch 22/25
440/440 [==============================] - 46s 102ms/step - loss: 0.0081 - acc: 0.9646 - categorical_accuracy: 0.9646 - val_loss: 0.0586 - val_acc: 0.6900 - val_categorical_accuracy: 0.6900
Epoch 23/25
440/440 [==============================] - 46s 102ms/step - loss: 0.0086 - acc: 0.9610 - categorical_accuracy: 0.9610 - val_loss: 0.0555 - val_acc: 0.7107 - val_categorical_accuracy: 0.7107
Epoch 24/25
440/440 [==============================] - 47s 103ms/step - loss: 0.0082 - acc: 0.9637 - categorical_accuracy: 0.9637 - val_loss: 0.0519 - val_acc: 0.7227 - val_categorical_accuracy: 0.7227
Epoch 25/25
440/440 [==============================] - 47s 104ms/step - loss: 0.0066 - acc: 0.9719 - categorical_accuracy: 0.9719 - val_loss: 0.0443 - val_acc: 0.7573 - val_categorical_accuracy: 0.7573
<tensorflow.python.keras.callbacks.History at 0x7fdb03c01d90>
[25]
classifier_fixed.evaluate(ds_test)
47/47 [==============================] - 2s 45ms/step - loss: 0.0443 - acc: 0.7573 - categorical_accuracy: 0.7573
[0.04430024325847626, 0.7573333382606506, 0.7573333382606506]
Model sin fijar
[33]
# Obtain features from middle layer
autoencoder = k.models.load_model('./autoencoder/ckpt/autoencoder-24-0.0029.hdf5')

encoder = k.Model(inputs=autoencoder.input,
                  outputs=autoencoder.get_layer('encoder').output)

encoder.summary()

classifier_trainable = model(encoder, trainable=True)


Model: "model_3"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 128, 128, 3)]     0         
_________________________________________________________________
conv2d (Conv2D)              (None, 128, 128, 128)     3584      
_________________________________________________________________
batch_normalization (BatchNo (None, 128, 128, 128)     512       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 64, 64, 128)       0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 64, 64, 64)        73792     
_________________________________________________________________
batch_normalization_1 (Batch (None, 64, 64, 64)        256       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 32, 32, 64)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 32, 32, 32)        18464     
_________________________________________________________________
batch_normalization_2 (Batch (None, 32, 32, 32)        128       
_________________________________________________________________
encoder (MaxPooling2D)       (None, 16, 16, 32)        0         
=================================================================
Total params: 96,736
Trainable params: 96,288
Non-trainable params: 448
_________________________________________________________________
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
model_3 (Functional)         (None, 16, 16, 32)        96736     
_________________________________________________________________
flatten_2 (Flatten)          (None, 8192)              0         
_________________________________________________________________
dense_18 (Dense)             (None, 4096)              33558528  
_________________________________________________________________
dense_19 (Dense)             (None, 4096)              16781312  
_________________________________________________________________
dense_20 (Dense)             (None, 2042)              8366074   
_________________________________________________________________
dense_21 (Dense)             (None, 2042)              4171806   
_________________________________________________________________
dense_22 (Dense)             (None, 1024)              2092032   
_________________________________________________________________
dense_23 (Dense)             (None, 1024)              1049600   
_________________________________________________________________
dense_24 (Dense)             (None, 512)               524800    
_________________________________________________________________
dense_25 (Dense)             (None, 512)               262656    
_________________________________________________________________
dense_26 (Dense)             (None, 8)                 4104      
=================================================================
Total params: 66,907,648
Trainable params: 66,907,200
Non-trainable params: 448
_________________________________________________________________
[34]
classifier_trainable.fit(ds_train,
                epochs=25,
                validation_data=ds_test, 
                callbacks=[
                           k.callbacks.TensorBoard(log_dir="./classifier-trainable/logs"), 
                           k.callbacks.ModelCheckpoint('./classifier-trainable/ckpt/classifier-trainable-{epoch:02d}-{val_loss:.4f}.hdf5', 
                                                       monitor='val_loss', 
                                                       save_
Epoch 1/25
440/440 [==============================] - 70s 153ms/step - loss: 0.1016 - acc: 0.2749 - categorical_accuracy: 0.2749 - val_loss: 0.0774 - val_acc: 0.5660 - val_categorical_accuracy: 0.5660
Epoch 2/25
440/440 [==============================] - 70s 154ms/step - loss: 0.0679 - acc: 0.6353 - categorical_accuracy: 0.6353 - val_loss: 0.0629 - val_acc: 0.6347 - val_categorical_accuracy: 0.6347
Epoch 3/25
440/440 [==============================] - 70s 156ms/step - loss: 0.0499 - acc: 0.7313 - categorical_accuracy: 0.7313 - val_loss: 0.0635 - val_acc: 0.6333 - val_categorical_accuracy: 0.6333
Epoch 4/25
440/440 [==============================] - 69s 153ms/step - loss: 0.0432 - acc: 0.7642 - categorical_accuracy: 0.7642 - val_loss: 0.0577 - val_acc: 0.6740 - val_categorical_accuracy: 0.6740
Epoch 5/25
440/440 [==============================] - 70s 155ms/step - loss: 0.0392 - acc: 0.7851 - categorical_accuracy: 0.7851 - val_loss: 0.0560 - val_acc: 0.6713 - val_categorical_accuracy: 0.6713
Epoch 6/25
440/440 [==============================] - 69s 155ms/step - loss: 0.0353 - acc: 0.8153 - categorical_accuracy: 0.8153 - val_loss: 0.0509 - val_acc: 0.7067 - val_categorical_accuracy: 0.7067
Epoch 7/25
440/440 [==============================] - 70s 155ms/step - loss: 0.0325 - acc: 0.8321 - categorical_accuracy: 0.8321 - val_loss: 0.0447 - val_acc: 0.7480 - val_categorical_accuracy: 0.7480
Epoch 8/25
440/440 [==============================] - 70s 156ms/step - loss: 0.0298 - acc: 0.8438 - categorical_accuracy: 0.8438 - val_loss: 0.0484 - val_acc: 0.7247 - val_categorical_accuracy: 0.7247
Epoch 9/25
440/440 [==============================] - 71s 157ms/step - loss: 0.0272 - acc: 0.8609 - categorical_accuracy: 0.8609 - val_loss: 0.0430 - val_acc: 0.7547 - val_categorical_accuracy: 0.7547
Epoch 10/25
440/440 [==============================] - 72s 160ms/step - loss: 0.0256 - acc: 0.8739 - categorical_accuracy: 0.8739 - val_loss: 0.0517 - val_acc: 0.7080 - val_categorical_accuracy: 0.7080
Epoch 11/25
440/440 [==============================] - 71s 157ms/step - loss: 0.0217 - acc: 0.8964 - categorical_accuracy: 0.8964 - val_loss: 0.0519 - val_acc: 0.6940 - val_categorical_accuracy: 0.6940
Epoch 12/25
440/440 [==============================] - 71s 159ms/step - loss: 0.0186 - acc: 0.9150 - categorical_accuracy: 0.9150 - val_loss: 0.0461 - val_acc: 0.7393 - val_categorical_accuracy: 0.7393
Epoch 13/25
440/440 [==============================] - 71s 157ms/step - loss: 0.0172 - acc: 0.9191 - categorical_accuracy: 0.9191 - val_loss: 0.0506 - val_acc: 0.6967 - val_categorical_accuracy: 0.6967
Epoch 14/25
440/440 [==============================] - 71s 159ms/step - loss: 0.0157 - acc: 0.9282 - categorical_accuracy: 0.9282 - val_loss: 0.0600 - val_acc: 0.6707 - val_categorical_accuracy: 0.6707
Epoch 15/25
440/440 [==============================] - 71s 158ms/step - loss: 0.0135 - acc: 0.9399 - categorical_accuracy: 0.9399 - val_loss: 0.0440 - val_acc: 0.7547 - val_categorical_accuracy: 0.7547
Epoch 16/25
440/440 [==============================] - 71s 157ms/step - loss: 0.0161 - acc: 0.9219 - categorical_accuracy: 0.9219 - val_loss: 0.0527 - val_acc: 0.7153 - val_categorical_accuracy: 0.7153
Epoch 17/25
440/440 [==============================] - 71s 157ms/step - loss: 0.0148 - acc: 0.9282 - categorical_accuracy: 0.9282 - val_loss: 0.0428 - val_acc: 0.7540 - val_categorical_accuracy: 0.7540
Epoch 18/25
440/440 [==============================] - 71s 157ms/step - loss: 0.0135 - acc: 0.9375 - categorical_accuracy: 0.9375 - val_loss: 0.0620 - val_acc: 0.6633 - val_categorical_accuracy: 0.6633
Epoch 19/25
440/440 [==============================] - 72s 160ms/step - loss: 0.0097 - acc: 0.9572 - categorical_accuracy: 0.9572 - val_loss: 0.0570 - val_acc: 0.6867 - val_categorical_accuracy: 0.6867
Epoch 20/25
440/440 [==============================] - 71s 158ms/step - loss: 0.0111 - acc: 0.9478 - categorical_accuracy: 0.9478 - val_loss: 0.0529 - val_acc: 0.6953 - val_categorical_accuracy: 0.6953
Epoch 21/25
440/440 [==============================] - 71s 158ms/step - loss: 0.0080 - acc: 0.9667 - categorical_accuracy: 0.9667 - val_loss: 0.0510 - val_acc: 0.7207 - val_categorical_accuracy: 0.7207
Epoch 22/25
440/440 [==============================] - 71s 157ms/step - loss: 0.0097 - acc: 0.9563 - categorical_accuracy: 0.9563 - val_loss: 0.0638 - val_acc: 0.6693 - val_categorical_accuracy: 0.6693
Epoch 23/25
440/440 [==============================] - 71s 157ms/step - loss: 0.0089 - acc: 0.9589 - categorical_accuracy: 0.9589 - val_loss: 0.0672 - val_acc: 0.6480 - val_categorical_accuracy: 0.6480
Epoch 24/25
440/440 [==============================] - 69s 154ms/step - loss: 0.0056 - acc: 0.9787 - categorical_accuracy: 0.9787 - val_loss: 0.0766 - val_acc: 0.6260 - val_categorical_accuracy: 0.6260
Epoch 25/25
440/440 [==============================] - 70s 155ms/step - loss: 0.0065 - acc: 0.9720 - categorical_accuracy: 0.9720 - val_loss: 0.0683 - val_acc: 0.6553 - val_categorical_accuracy: 0.6553
<tensorflow.python.keras.callbacks.History at 0x7fdb02cec290>
[35]
classifier_trainable.evaluate(ds_test)
47/47 [==============================] - 2s 43ms/step - loss: 0.0683 - acc: 0.6553 - categorical_accuracy: 0.6553
[0.06830702722072601, 0.6553333401679993, 0.6553333401679993]
[62]
def full_model(input_shape=(128, 128, 3), kernel_size=(3,3), pool_size=(2,2)):
    model = k.Sequential([
        layers.Input(shape=input_shape),
        layers.Conv2D(128, kernel_size, activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling2D(pool_size, padding='same'),
        layers.Conv2D(64, kernel_size, activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling2D(pool_size, padding='same'),
        layers.C
[63]
full_model = full_model()

full_model.summary()
Model: "sequential_10"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_28 (Conv2D)           (None, 128, 128, 128)     3584      
_________________________________________________________________
batch_normalization_27 (Batc (None, 128, 128, 128)     512       
_________________________________________________________________
max_pooling2d_16 (MaxPooling (None, 64, 64, 128)       0         
_________________________________________________________________
conv2d_29 (Conv2D)           (None, 64, 64, 64)        73792     
_________________________________________________________________
batch_normalization_28 (Batc (None, 64, 64, 64)        256       
_________________________________________________________________
max_pooling2d_17 (MaxPooling (None, 32, 32, 64)        0         
_________________________________________________________________
conv2d_30 (Conv2D)           (None, 32, 32, 32)        18464     
_________________________________________________________________
batch_normalization_29 (Batc (None, 32, 32, 32)        128       
_________________________________________________________________
encoder (MaxPooling2D)       (None, 16, 16, 32)        0         
_________________________________________________________________
flatten_10 (Flatten)         (None, 8192)              0         
_________________________________________________________________
dense_90 (Dense)             (None, 4096)              33558528  
_________________________________________________________________
dense_91 (Dense)             (None, 4096)              16781312  
_________________________________________________________________
dense_92 (Dense)             (None, 2042)              8366074   
_________________________________________________________________
dense_93 (Dense)             (None, 2042)              4171806   
_________________________________________________________________
dense_94 (Dense)             (None, 1024)              2092032   
_________________________________________________________________
dense_95 (Dense)             (None, 1024)              1049600   
_________________________________________________________________
dense_96 (Dense)             (None, 512)               524800    
_________________________________________________________________
dense_97 (Dense)             (None, 512)               262656    
_________________________________________________________________
dense_98 (Dense)             (None, 8)                 4104      
=================================================================
Total params: 66,907,648
Trainable params: 66,907,200
Non-trainable params: 448
_________________________________________________________________